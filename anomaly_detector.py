import pandas as pd
from adtk.data import validate_series
from adtk.visualization import plot
from adtk.detector import OutlierDetector
from sklearn.neighbors import LocalOutlierFactor
import time
import os
import sys
import logging
from datetime import datetime

# Path to the log file generated by the physical process simulator
# Use local path when running locally, Docker path when in container
if os.path.exists("/usr/src/app/logs/power_flow.log"):
    LOG_FILE = "/usr/src/app/logs/power_flow.log"
    ANOMALY_LOG_FILE = "/usr/src/app/logs/anomaly_detector.log"
    LOG_DIR = "/usr/src/app/logs"
else:
    LOG_FILE = "./logs/power_flow.log"
    ANOMALY_LOG_FILE = "./logs/anomaly_detector.log"
    LOG_DIR = "./logs"

def setup_logging():
    """Set up logging to both console and file."""
    # Ensure log directory exists
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # Create logger
    logger = logging.getLogger('anomaly_detector')
    logger.setLevel(logging.INFO)
    
    # Clear any existing handlers
    logger.handlers.clear()
    
    # Create formatters
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    simple_formatter = logging.Formatter('%(message)s')
    
    # File handler for detailed logs
    file_handler = logging.FileHandler(ANOMALY_LOG_FILE, mode='a')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(detailed_formatter)
    logger.addHandler(file_handler)
    
    # Console handler for simple output
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(simple_formatter)
    logger.addHandler(console_handler)
    
    return logger

def monitor_and_detect():
    """
    Monitors the power flow log file and detects anomalies.
    """
    # Set up logging
    logger = setup_logging()
    
    logger.info("=" * 60)
    logger.info("ANOMALY DETECTION AGENT STARTED")
    logger.info("=" * 60)
    logger.info(f"Monitoring log file: {LOG_FILE}")
    logger.info(f"Writing anomaly logs to: {ANOMALY_LOG_FILE}")

    # Ensure the log directory exists
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

    while not os.path.exists(LOG_FILE):
        logger.info(f"Log file not found at {LOG_FILE}, waiting...")
        time.sleep(2)

    logger.info("Log file found. Starting monitoring...")

    while True:
        try:
            # Read the latest data from the log file
            # The log file has no headers, so we specify column names
            data = pd.read_csv(
                LOG_FILE,
                header=None,  # No header row in the file
                names=["timestamp", "loading_percent"],  # Specify column names
                index_col="timestamp",
                parse_dates=["timestamp"]
            )

            if data.empty:
                logger.info("No data in log file yet, waiting...")
                time.sleep(10)
                continue

            # Remove rows with 0.0 values (breaker open states) for better anomaly detection
            # We only want to detect anomalies in actual power flow data
            active_data = data[data["loading_percent"] > 0]
            
            if len(active_data) < 10:  # Need minimum data points for anomaly detection
                logger.info(f"Insufficient active data points ({len(active_data)}), waiting for more data...")
                time.sleep(10)
                continue

            # The ADTK library requires a validated time series
            s = validate_series(active_data["loading_percent"])

            # --- Anomaly Detection using Local Outlier Factor ---
            # Convert series to DataFrame for ADTK
            df_data = pd.DataFrame(s, columns=['loading_percent'])
            
            # This model identifies outliers based on the local density of data points.
            outlier_detector = OutlierDetector(LocalOutlierFactor(contamination=0.1))
            anomalies = outlier_detector.fit_detect(df_data)

            # Check if any anomalies were detected (anomalies is a Series with True/False values)
            if isinstance(anomalies, pd.Series):
                anomaly_timestamps = anomalies[anomalies == True].index
            else:
                anomaly_timestamps = []
            
            if len(anomaly_timestamps) > 0:
                logger.info("")
                logger.info("ðŸš¨ ANOMALY DETECTED!")
                logger.info("Unusual power line loading detected at the following times:")
                for timestamp in anomaly_timestamps:
                    loading_value = s.loc[timestamp]
                    logger.info(f"  âš¡ {timestamp}: {loading_value:.2f}% loading")
                logger.info("ðŸ“‹ END ANOMALY REPORT")
                logger.info("")
            else:
                current_reading = s.iloc[-1] if len(s) > 0 else 0
                timestamp_str = pd.Timestamp.now().strftime('%H:%M:%S')
                logger.info(f"âœ… ({timestamp_str}) System normal. Current loading: {current_reading:.1f}%")

        except pd.errors.EmptyDataError:
            logger.info("Log file is empty, waiting for data...")
        except pd.errors.ParserError as e:
            logger.error(f"Error parsing CSV data: {e}")
        except FileNotFoundError:
            logger.error(f"Log file not found at {LOG_FILE}")
        except Exception as e:
            logger.error(f"Error processing data: {e}")
            # Print more details for debugging
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

        time.sleep(10)  # Check for anomalies every 10 seconds

if __name__ == "__main__":
    monitor_and_detect()